# Modular RAG MCP Server - Configuration
# This is the main configuration file for the MCP Server.
# Edit this file to configure LLM, Embedding, VectorStore, and other services.

# =============================================================================
# LLM Configuration
# =============================================================================
llm:
  provider: "azure"  # Options: openai, azure, ollama, deepseek
  model: "gpt-4o"
  deployment_name: "gpt-4o"
  azure_endpoint: ""  # Set your Azure OpenAI endpoint here
  api_version: "2024-02-15-preview"
  api_key: ""  # Set your Azure OpenAI API key here or use AZURE_OPENAI_API_KEY env var
  temperature: 0.0
  max_tokens: 4096

# =============================================================================
# Embedding Configuration
# =============================================================================
embedding:
  provider: "openai"  # Options: openai, local
  model: "text-embedding-3-small"
  dimensions: 1536
  # api_key: ${OPENAI_API_KEY}

# =============================================================================
# Vector Store Configuration
# =============================================================================
vector_store:
  provider: "chroma"  # Options: chroma, qdrant, pinecone
  persist_directory: "./data/db/chroma"
  collection_name: "knowledge_hub"

# =============================================================================
# Retrieval Configuration
# =============================================================================
retrieval:
  dense_top_k: 20
  sparse_top_k: 20
  fusion_top_k: 10
  rrf_k: 60  # RRF constant

# =============================================================================
# Rerank Configuration
# =============================================================================
rerank:
  enabled: false
  provider: "none"  # Options: none, cross_encoder, llm
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_k: 5

# =============================================================================
# Evaluation Configuration
# =============================================================================
evaluation:
  enabled: false
  provider: "custom"  # Options: ragas, deepeval, custom
  metrics:
    - "hit_rate"
    - "mrr"
    - "faithfulness"

# =============================================================================
# Observability Configuration
# =============================================================================
observability:
  log_level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  trace_enabled: true
  trace_file: "./logs/traces.jsonl"
  structured_logging: true

# =============================================================================
# Ingestion Configuration
# =============================================================================
ingestion:
  chunk_size: 1000
  chunk_overlap: 200
  splitter: "recursive"  # Options: recursive, semantic, fixed_length
  batch_size: 100
  
# Chunk Refiner Configuration (C5)
chunk_refiner:
  use_llm: false  # Set to true to enable LLM-based refinement (requires LLM config)
  # When use_llm is false or LLM call fails, falls back to rule-based refinement
